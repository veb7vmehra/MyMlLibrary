{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clean_Sheet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwQm3UJ_haD8"
      },
      "source": [
        "#run every time\n",
        "#mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykf7y_RlhsAG"
      },
      "source": [
        "#run once\n",
        "#download your kaggle config file from account and upload it to drive with following command.\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4__3KOlTiEd-"
      },
      "source": [
        "#run once\n",
        "#install kaggle and synchronize it with your account with the help of config file\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z79olPTifwe"
      },
      "source": [
        "#run once\n",
        "#download any kaggle database to any location in google drive\n",
        "#here download \"ieee-fraud-detection\" database at \"/content/gdrive/My\\Drive/kaggle/fraud-detection\"\n",
        "!kaggle competitions download -c ieee-fraud-detection -p /content/gdrive/My\\Drive/kaggle/fraud-detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DTPTqCJi4h6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "415e40c9-bebb-4d8d-e630-880b9277e5d9"
      },
      "source": [
        "#run everytime\n",
        "#changing the main directory of colab file \n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/kaggle/fraud-detection')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bfa11436c72b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#changing the main directory of colab file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/kaggle/fraud-detection'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/kaggle/fraud-detection'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf_UFXvkjI7u"
      },
      "source": [
        "#run once\n",
        "#unzipping the data into the newly created data folder\n",
        "!mkdir data\n",
        "!unzip -q ieee-fraud-detection.zip -d data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlrjuDXNjZaS"
      },
      "source": [
        "#run everytime\n",
        "#download correct version of xgboost from here \"https://drive.google.com/file/d/1vyHsOalOLz7bNl2fvSvUKqgnBjivuHe4/view?usp=sharing\"\n",
        "#following command will update the xgboost version to bring it in runnable state\n",
        "!pip uninstall xgboost\n",
        "!pip install xgboost-0.81-py2.py3-none-manylinux1_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5v7QTb1kJHJ"
      },
      "source": [
        "#run everytime\n",
        "#importing all the dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os,gc,re\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,auc,confusion_matrix\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "import time\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "colors = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5BLFuI1kVRr"
      },
      "source": [
        "#run everytime\n",
        "#installing/checking nvidia drivers\n",
        "#if gives error try changing the runtime type to GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw5rsR2GkwKt"
      },
      "source": [
        "#run once\n",
        "#get a look at the sizes of downloaded data files\n",
        "for f in os.listdir('./data'):\n",
        "  print(f.ljust(30) + str(round(os.path.getsize('./data/' + f) / 1000000, 2)) + 'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhXtSioFlFLx"
      },
      "source": [
        "So right now we will be working with the IEEE-CIS Fraud Detection which contains the data provided by Vesta Coporation. Vesta Corporation who is one of the forerunners in guaranteed e-commerce payment solutions provided this dataset. The database in itself has 4 files, transaction data for training and testing and identity data for training and testing. By looking at the size of our database we can see that the training and the testing database is more or less equally divided and that’s why there are quite really low chances of overfitting of our model. Now to get an overview of our database let us look into the head or what we call the initial values of our training database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X97ROaa2mgvw"
      },
      "source": [
        "#run once\n",
        "df_train_transaction = pd.read_csv('data/train_transaction.csv')\n",
        "df_train_identity = pd.read_csv('data/train_identity.csv')\n",
        "print(f'Shape of transaction train data: {df_train_transaction.shape}')\n",
        "print(f'Shape of identity train data: {df_train_identity.shape}')\n",
        "df_train_transaction.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYSiGMfYm2Y9"
      },
      "source": [
        "#run once\n",
        "df_train_identity.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keC5Ro9Nm6s3"
      },
      "source": [
        "#run once\n",
        "df_test_transaction = pd.read_csv('data/test_transaction.csv')\n",
        "df_test_identity = pd.read_csv('data/test_identity.csv')\n",
        "df_test_transaction.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpEYZ6Gxm9t9"
      },
      "source": [
        "#run once\n",
        "df_test_identity.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlHVliSMv3PA"
      },
      "source": [
        "It is important to note that all transactions do not have corresponding identity information. Actually we don't have access to exact information about columns and what they represent in the database. Mostly it is because of security purposes as we are handling transaction data but we would try to predict what kind of information is actually provided by the columns or the features of the database. Let us get familiarity with various columns present:\n",
        "\n",
        "**Transaction Database**\n",
        "*   Transaction id: Id related to the transaction\n",
        "*   TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n",
        "* TransactionAMT: transaction payment amount in USD\n",
        "* ProductCD [Categorical]: product code(the product for each transaction)\n",
        "* Card 1–6 [Categorical]: payment card related information like card type, country etc\n",
        "* Addr1, addr2 [Categorical]: address information\n",
        "* Dist1,dist2: some distance information\n",
        "* P_emaildomain [Categorical]: email domain of purchaser.\n",
        "* R_emaildomain [Categorical]: email domain of the recipient.\n",
        "* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
        "* D1-D15: time delta, such as days between the previous transactions, etc.\n",
        "* M1-M9 [Categorical]: match, such as names on card and address, etc.\n",
        "* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
        "**Identity Database**\n",
        "* Transaction id: Id related to the transaction\n",
        "* DeviceType [Categorical]: Type of device used for the transaction\n",
        "* DeviceInfo [Categorical]: More information about device used\n",
        "* id 1–38 [Categorical+numeric]: network connection information,browser information etc (id 12–38 are categorical information)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DP8ul6BxdEP"
      },
      "source": [
        "#run once\n",
        "# we can observe that ids have different name in test dataset, let us correct that.\n",
        "id_cols = [col for col in df_test_identity.columns if col[0]+col[1] == 'id']\n",
        "rename_cols = {i:'id_'+str(i[-2]+i[-1]) for i in id_cols}\n",
        "df_test_identity = df_test_identity.rename(columns=rename_cols)\n",
        "\n",
        "\n",
        "print(f'Shape of transaction train data: {df_test_transaction.shape}')\n",
        "print(f'Shape of identity train data: {df_test_identity.shape}')\n",
        "\n",
        "df_train_transaction.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgOgSd6jyRQw"
      },
      "source": [
        "#run once\n",
        "df_train_identity.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3eUVq1WyWMR"
      },
      "source": [
        "#run once\n",
        "#now it is important to combine transaction and id dataset to analyse data better and quickly\n",
        "\n",
        "df_train_transaction = pd.read_csv('data/train_transaction.csv')\n",
        "df_train_identity = pd.read_csv('data/train_identity.csv')\n",
        "\n",
        "df_train = df_train_transaction.merge(df_train_identity,on=['TransactionID'],how='left')\n",
        "\n",
        "df_test_transaction = pd.read_csv('data/test_transaction.csv')\n",
        "df_test_identity = pd.read_csv('data/test_identity.csv')\n",
        "\n",
        "id_cols = [col for col in df_test_identity.columns if col[0]+col[1] == 'id']\n",
        "rename_cols = {i:'id_'+str(i[-2]+i[-1]) for i in id_cols}\n",
        "df_test_identity = df_test_identity.rename(columns=rename_cols)\n",
        "df_test = df_test_transaction.merge(df_test_identity,on=['TransactionID'],how='left')\n",
        "\n",
        "df_train.to_csv('data/train_combined.csv',index=False)\n",
        "df_test.to_csv('data/test_combined.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62xHeHkayemS"
      },
      "source": [
        "#run everytime\n",
        "#importing the combined database directly\n",
        "df_train = pd.read_csv('data/train_combined.csv')\n",
        "df_test = pd.read_csv('data/test_combined.csv')\n",
        "print(df_train.shape)\n",
        "print(df_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCLOdHC4yxKl"
      },
      "source": [
        "#run once\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVKzyHjgy6X_"
      },
      "source": [
        "#run once\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2yg7J5My94w"
      },
      "source": [
        "#run once\n",
        "#all the database have some missing values and it is always preffered to deal with them at initial stage.\n",
        "#function to find out the columns with highest percentage of missing values\n",
        "\n",
        "def top_missing_cols(df,n=10,thresh=80):\n",
        "  dff = (df.isnull().sum()/df.shape[0])*100\n",
        "  dff = dff.reset_index()\n",
        "  dff.columns = ['col','missing_percent']\n",
        "  dff = dff.sort_values(by=['missing_percent'],ascending=False).reset_index(drop=True)\n",
        "  print(f'There are {df.isnull().any().sum()} columns in this dataset with missing values.')\n",
        "  print(f'There are {dff[dff[\"missing_percent\"] > thresh].shape[0]} columns with missing percent values than {thresh}%')\n",
        "  if n:\n",
        "    return dff.head(n)\n",
        "  else:\n",
        "    return dff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwkQ5i1szWw7"
      },
      "source": [
        "#run once\n",
        "top_missing_cols(df_train,thresh=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrQpg8rUzbEy"
      },
      "source": [
        "#run once\n",
        "top_missing_cols(df_test,thresh=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QT8X62Kzh36"
      },
      "source": [
        "We can see that columns like id_21,id_22,id_23,id_24,id_25,id_26,id_27,id_01,id_07,id_08 have more than 99 percent missing values. Its better we drop those features. Now we will start with the EDA of the dataset. It's not ideal to explain the analysis of each and every feature here. I will explain some of my interesting observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWrQbXj2z25w"
      },
      "source": [
        "#run once\n",
        "#Analysis of Is_Fraud column. This is the column which we need to predict\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(10,6))\n",
        "sns.countplot(df_train['isFraud'])\n",
        "axes.title.set_text('Target')\n",
        "total = float(df_train['isFraud'].shape[0])  \n",
        "for p in axes.patches:\n",
        "    height = p.get_height()\n",
        "    axes.text(p.get_x()+p.get_width()/2.,\n",
        "            height + 3,\n",
        "            '{:1.2f}%'.format(height*100/total),\n",
        "            ha=\"center\") \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4ctF0Xx0Onm"
      },
      "source": [
        "**Observations**\n",
        "\n",
        "As expected we can see that class is heavily imbalanced. Here 96.5% of transactions are not fraud and the rest 3.5% of transactions are fraudulent. We will choose Area under the ROC curve (AUC) as a metric for our ML problem. AUC is the area under the ROC curve. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. The ROC curve is plotted with TPR against the FPR where TPR is on the y-axis and FPR is on the x-axis.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW3be7BP0kMj"
      },
      "source": [
        "#run once\n",
        "#Analysis of Transaction Date. We will check if it is continuous or not. This is important for train test split.\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12,5))\n",
        "sns.histplot(df_train['TransactionDT'], color='g')\n",
        "sns.histplot(df_test['TransactionDT'],color='r')\n",
        "axes.title.set_text('Train - Test Transaction date - distribution')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEZMDWXI1HEj"
      },
      "source": [
        "**Observations:**\n",
        "There is a slight gap in between, but otherwise, the training set is from an earlier period of time and test data from a later period of time. This can impact train validation split or cross-validation techniques should be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSfr_3iW1fpj"
      },
      "source": [
        "#run once\n",
        "#getting the percentage of fraud transaction per day of weak\n",
        "\n",
        "df_train['dayofweek'] = (df_train['TransactionDT']//(60*60*24)-1)%7\n",
        "tmp = df_train[['isFraud','dayofweek']].groupby(by=['dayofweek']).mean().reset_index() \\\n",
        "            .rename(columns={'isFraud':'Percentage fraud transactions'})\n",
        "\n",
        "tmp_count = df_train[['TransactionID','dayofweek']].groupby(by=['dayofweek']).count().reset_index() \\\n",
        "            .rename(columns={'TransactionID':'Number of transactions'})\n",
        "tmp = tmp.merge(tmp_count,on=['dayofweek'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(12,5))\n",
        "axes = sns.lineplot(x=tmp['dayofweek'],y=tmp['Percentage fraud transactions'],color='r')\n",
        "axes2 = axes.twinx()\n",
        "axes2 = sns.barplot(x=tmp['dayofweek'],y=tmp['Number of transactions'],palette=\"flare\")\n",
        "axes.set_title('Fraud transaction vs dayofweek')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3saw29t1pm8"
      },
      "source": [
        "#run once\n",
        "#getting the percentage of fraud transaction per hour of week\n",
        "\n",
        "df_train['hour'] = (df_train['TransactionDT']//(60*60))%24\n",
        "tmp = df_train[['isFraud','hour']].groupby(by=['hour']).mean().reset_index() \\\n",
        "            .rename(columns={'isFraud':'Percentage fraud transactions'})\n",
        "\n",
        "tmp_count = df_train[['TransactionID','hour']].groupby(by=['hour']).count().reset_index() \\\n",
        "            .rename(columns={'TransactionID':'Number of transactions'})\n",
        "tmp = tmp.merge(tmp_count,on=['hour'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(12,5))\n",
        "axes = sns.lineplot(x=tmp['hour'],y=tmp['Percentage fraud transactions'],color='r')\n",
        "axes2 = axes.twinx()\n",
        "axes2 = sns.barplot(x=tmp['hour'],y=tmp['Number of transactions'],palette='flare')\n",
        "axes.set_title('Fraud transaction(no of transactions) vs hour')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rkQq_cm11FM"
      },
      "source": [
        "**Observations:** We can see that on the 3rd fay fraudulent transactions are very less, similarly, in the 7th hour, the percentage of fraudulent transactions is high compared to other hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eLWIYD41_to"
      },
      "source": [
        "#run once\n",
        "#checking exceptionally high amount transactions\n",
        "\n",
        "fig, axes = plt.subplots(1,2,figsize=(15,5))\n",
        "\n",
        "sns.scatterplot(y=df_train['TransactionAmt'],x=df_train['TransactionDT'],hue=df_train['isFraud'],ax=axes[0])\n",
        "axes[0].title.set_text('Transcation Amount - Train')\n",
        "\n",
        "sns.scatterplot(y=df_test['TransactionAmt'],x=df_test['TransactionDT'],ax=axes[1])\n",
        "axes[1].title.set_text('Transcation Amount - Test')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkJNrpRz2LTF"
      },
      "source": [
        "#run once\n",
        "#as most transaction are having low amount of money to get better insight let's try comparing the log of transactions\n",
        "\n",
        "test_amt = np.log(df_test[['TransactionAmt']])\n",
        "\n",
        "dff_fraud = df_train[df_train['isFraud'] == 1]\n",
        "dff_notfraud = df_train[df_train['isFraud'] == 0]\n",
        "\n",
        "dff_fraud['TransactionAmt'] = np.log(dff_fraud['TransactionAmt'])\n",
        "dff_notfraud['TransactionAmt'] = np.log(dff_notfraud['TransactionAmt'])\n",
        "\n",
        "\n",
        "fig,axes = plt.subplots(1,2,figsize=(15,8))\n",
        "sns.distplot(dff_notfraud['TransactionAmt'],ax=axes[0],label='not fraud')\n",
        "sns.distplot(dff_fraud['TransactionAmt'],ax=axes[0],label='fraud')\n",
        "axes[0].title.set_text('Log(Fraud transaction distribution) Train')\n",
        "axes[0].legend()\n",
        "\n",
        "sns.distplot(test_amt,ax=axes[1])\n",
        "axes[1].title.set_text('Log(Fraud transaction distribution) Test')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plSUVYfF2Ra6"
      },
      "source": [
        "**Observations:** We can see that there is one point in train data where the amount > 30000. It's better we remove the outlier since it can affect our models(especially distance-based algorithms like logistic regression, knn etc.) in prediction. Also, Outliers like this can cause an overfitting problem. For instance, tree-based models can put these outliers in leaf nodes, which are noise and not part of a general pattern. Therefore, I decided to remove the values larger than 30,000 in the training set.\n",
        "\n",
        "It seems that the transactions with ‘LogTransactionAmt’ larger than 5.5 (244 dollars) and smaller than 3.3 (27 dollars) have a higher frequency and probability density of being fraudulent. On the other hand, the ones with ‘LogTransactionAmt’ from 3.3 to 5.5 have a higher chance of being legit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owj-pzEV2fBq"
      },
      "source": [
        "#run everytime\n",
        "#removing exceptionally high values from database\n",
        "\n",
        "df_train = df_train[df_train['TransactionAmt'] < 30000]\n",
        "df_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1L8-B7b20FX"
      },
      "source": [
        "#run once\n",
        "#analysing the product type column\n",
        "\n",
        "# idea from https://www.kaggle.com/gpreda/home-credit-default-risk-extensive-eda\n",
        "def hor_plot(feat,df=df_train,label_rotation=False,shape=(12,8)):\n",
        "    \n",
        "    val_cnts = df[feat].value_counts()\n",
        "    df1 = pd.DataFrame({feat: val_cnts.index,'Number of units': val_cnts.values})\n",
        "    # Calculate the percentage of target=1 per category value.For that what we did is we took the mean value of TARGET\n",
        "    percent = df[[feat, 'isFraud']].groupby([feat],as_index=False).mean()\n",
        "    percent.sort_values(by='isFraud', ascending=False, inplace=True)\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=shape)\n",
        "    s = sns.barplot(ax=ax1, x = feat, y=\"Number of units\",data=df1)\n",
        "    if(label_rotation):\n",
        "        s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
        "    \n",
        "    s = sns.barplot(ax=ax2, x = feat, y='isFraud', order=percent[feat], data=percent)\n",
        "    if(label_rotation):\n",
        "        s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
        "    \n",
        "    plt.ylabel('Percent of Fraud transactions [Target with value 1]', fontsize=10)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "    plt.show();\n",
        "\n",
        "\n",
        "hor_plot(df=df_train,feat='ProductCD')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "debvSEmf3APs"
      },
      "source": [
        "We can see that among transactions related to product code C about 12 percent are fraud transactions. similarly close to 6% of transactions are fraud among product codes.\n",
        "\n",
        "We have D columns from D1 to D15. These represent time deltas such as days between previous transaction and current transaction, days between current and first transaction, etc. Even though they have not given a clear understanding regarding D cols, we got some insights after a good analysis. We assume D1 as the day since credit card usage has begun. Subtracting this from Transaction day will result in content value per client."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-tzJo6_4ZKK"
      },
      "source": [
        "#run once\n",
        "#Observing D1 before normalization\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.scatter(df_train['TransactionDT'],df_train['D1n'])\n",
        "plt.title(f'Original {col}')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel(f'{col}')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gMFvmmA4yxE"
      },
      "source": [
        "#run everytime\n",
        "#normalizing D1\n",
        "\n",
        "df_train['D1n'] =  df_train['D1'] - df_train.TransactionDT/np.float32(24*60*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBZV9kwF5Uy5"
      },
      "source": [
        "#run once\n",
        "#Observing D1 after normalization\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.scatter(df_train['TransactionDT'],df_train['D1n'])\n",
        "plt.title(f'Original {col}')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel(f'{col}')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H97r_LRJ57R6"
      },
      "source": [
        "#run once\n",
        "#Observing Card1 to Card6\n",
        "\n",
        "for c in cards:\n",
        "    if df_train[c].dtypes in ['int64','float64']:\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(10,8))\n",
        "        sns.histplot(df_train[c])\n",
        "        axes.title.set_text(f'Card {c}')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dmvoP6q6JNY"
      },
      "source": [
        "#run once\n",
        "hor_plot('card4',label_rotation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inu2vgOf6NmV"
      },
      "source": [
        "#run once\n",
        "hor_plot('card6')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2cy8CII5mK-"
      },
      "source": [
        "**Observing Card1 - Card6**\n",
        "\n",
        "These columns represent payment information related to cards. Some interesting observations from these features are:\n",
        "\n",
        "* There is a wide range of values in card1. If we see the distribution there are overlapping's. card1 alone cannot distinguish between fraudulent and nonfraudulent transactions. No missing values are there in card1.\n",
        "* A small percent of data is missing in card2. Similar to card1 a lot of unique values are present in card2 also. card3 has a low correlation with other card columns.\n",
        "* card 4 indicates which type of card do client uses visa, Mastercard, American Express, or discover. Similarly, card6 indicates the type of card- debit or credit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwNKaIdr_Sza"
      },
      "source": [
        "**Some Important Observations:**\n",
        "* **P-email domain and R-email domain:** These are email domains of purchaser and recipient respectively. About 76% of values are missing in the R-email domain. In purchaser email domains most mails are from gmail.com. Among those More fraud transactions comes from domain protonmail.com which is more than 90% which is a serious issue.\n",
        "* **addr1 and addr2:** These are some address-related information related to the client. Around 11% of data is missing from both addr1 and addr2. There are about 332 unique values in addr1 and 74 in the case of addr2.\n",
        "* **dist1 and dist2:** This might indicate the distance between transaction location and card owner address. This is just my assumption. We have about 93% of values missing in dist2. There is no correlation between dist1 and dist2. While dist2 is very weakly correlated with all other attributes.\n",
        "* **C1-C14:** These are information such as counting. C3 is somewhat different from other Cxx features. For c3, from box pot, it is clear that there is no value beyond 3 in train data for fraudulent transactions. For no fraudulent transactions, the value ranges from 0 to 26. Also, C3 is very weakly co-related with other Cxx columns.\n",
        "* **M1-M9:** More than 50 percent of values are missing among M columns. These features do not clearly distinguish between fraudulent and nonfraudulent transactions.\n",
        "* **V1-V399:** These are vesta-engineered features. These are masked information that can be ranking, counting, or other entity relations. There exist a strong correlation (>0.9) between many features. If possible It's better we reduce the number of these features as it can reduce the computational complexity of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIdknQTspiaw"
      },
      "source": [
        "#run everytime\n",
        "#Differing category columns from numeric ones\n",
        "\n",
        "# column details\n",
        "cat_cols = (['ProductCD'] + \n",
        "            ['card%d' % i for i in range(1, 7)] + \n",
        "            ['addr1', 'addr2', 'P_emaildomain', 'R_emaildomain'] + \n",
        "            ['M%d' % i for i in range(1, 10)] + \n",
        "            ['DeviceType', 'DeviceInfo'] +\n",
        "            ['id_%d' % i for i in range(12, 39)])\n",
        "\n",
        "\n",
        "type_map = {c: str for c in cat_cols}\n",
        "df_train[cat_cols] = df_train[cat_cols].astype(type_map, copy=False)\n",
        "df_test[cat_cols] = df_test[cat_cols].astype(type_map, copy=False)\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "id_cols = ['TransactionID', 'TransactionDT']\n",
        "target = 'isFraud'\n",
        "\n",
        "numeric_cols =  [\n",
        "    'TransactionAmt', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', \n",
        "    'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', \n",
        "    'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', \n",
        "    'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', \n",
        "    'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', \n",
        "    'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', \n",
        "    'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', \n",
        "    'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', \n",
        "    'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', \n",
        "    'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', \n",
        "    'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', \n",
        "    'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', \n",
        "    'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', \n",
        "    'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', \n",
        "    'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', \n",
        "    'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', \n",
        "    'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', \n",
        "    'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', \n",
        "    'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', \n",
        "    'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', \n",
        "    'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', \n",
        "    'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', \n",
        "    'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', \n",
        "    'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', \n",
        "    'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', \n",
        "    'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', \n",
        "    'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', \n",
        "    'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', \n",
        "    'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', \n",
        "    'V337', 'V338', 'V339', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', \n",
        "    'id_09', 'id_10', 'id_11'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW_zj3Fr0NiE"
      },
      "source": [
        "#run everytime runningRandomForest \n",
        "## Train and test split¶\n",
        "y_train_ = df_train['isFraud']\n",
        "X_train = df_train.drop(columns=['isFraud'])\n",
        "X_test = df_test.copy()\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYx4Xdtx0SCl"
      },
      "source": [
        "#it is usually preferred to reload the database for new model so we can delete recent one for now.\n",
        "\n",
        "del df_train,df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJHviBgi3Wv9"
      },
      "source": [
        "#run everytime running Random Forest\n",
        "# Label encoding all cat features\n",
        "for col in X_train.columns:\n",
        "    if col in cat_cols:\n",
        "        # label encode all cat columns\n",
        "        dff = pd.concat([X_train[col],X_test[col]])\n",
        "        dff,_ = pd.factorize(dff,sort=True)\n",
        "        if dff.max()>32000: \n",
        "            print(col,'needs int32 datatype')\n",
        "            \n",
        "        X_train[col] = dff[:len(X_train)].astype('int16')\n",
        "        X_test[col] = dff[len(X_train):].astype('int16')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU_LEPRW3kHH"
      },
      "source": [
        "#run everytime running Random Forest\n",
        "# Scaling numeric features\n",
        "cols = X_train.columns\n",
        "for col in cols:\n",
        "    if col not in cat_cols and col not in id_cols:\n",
        "        # min max scalar\n",
        "        dff = pd.concat([X_train[col],X_test[col]])\n",
        "        dff = (dff - dff.min())/(dff.max() - dff.min())\n",
        "        dff.fillna(-1,inplace=True)\n",
        "\n",
        "        X_train[col] = dff[:len(X_train)]\n",
        "        X_test[col] = dff[len(X_train):]\n",
        "\n",
        "del dff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h73-9Gpo3ofg"
      },
      "source": [
        "#run everytime running Random Forest\n",
        "# # train test split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# x_train,x_cv_,y_train,y_cv = train_test_split(X_train,y_train, stratify = y_train,test_size=0.3, random_state=40)\n",
        "\n",
        "idx_train = X_train.index[:int(X_train.shape[0]*0.75)]  \n",
        "idx_validation = X_train.index[int(X_train.shape[0]*0.75):]\n",
        "\n",
        "x_train,y_train = X_train.iloc[idx_train],y_train_.iloc[idx_train]\n",
        "x_cv_,y_cv = X_train.iloc[idx_validation],y_train_.iloc[idx_validation]\n",
        "\n",
        "x_train.to_csv('data/x_train.csv',index=False)\n",
        "x_cv_.to_csv('data/x_cv_.csv',index=False)\n",
        "y_train.to_csv('data/y_train.csv',index=False)\n",
        "y_cv.to_csv('data/y_cv.csv',index=False)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_cv_.shape)\n",
        "print(y_train.shape)\n",
        "print(y_cv.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztdgPw6b31LP"
      },
      "source": [
        "#run everytime running the Random forest\n",
        "#Actually running or fitting the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier()\n",
        "model.fit(x_train,y_train)\n",
        "y_train_pred = model.predict(x_train)\n",
        "y_test_pred = model.predict(x_cv_)\n",
        "\n",
        "train_fpr, train_tpr, thresholds = roc_curve(y_train, model.predict_proba(x_train)[:,1])\n",
        "test_fpr, test_tpr, thresholds = roc_curve(y_cv, model.predict_proba(x_cv_)[:,1])\n",
        "\n",
        "#Area under ROC curve\n",
        "print('Area under train roc {}'.format(auc(train_fpr, train_tpr)))\n",
        "print('Area under test roc {}'.format(auc(test_fpr, test_tpr)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy5fIkoy3_i6"
      },
      "source": [
        "#run everytime running Random Forest\n",
        "#Due to lower accuracy trying to figure out the perfect depth for random trees in model\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "clf = RandomForestClassifier(n_jobs= -1,class_weight='balanced')\n",
        "no_of_estimators = [5, 10, 50, 100, 120]\n",
        "maximumdepth = [1, 5, 7, 10, 15, 25, 30]\n",
        "parameters = {'n_estimators': no_of_estimators ,'max_depth':maximumdepth}\n",
        "model = RandomizedSearchCV(estimator=clf,  param_distributions=parameters, cv=3, n_iter=6, scoring='roc_auc')\n",
        "model.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZO22qow4LE0"
      },
      "source": [
        "#run everytime running Random Forest\n",
        "model.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veia1GSW4Svr"
      },
      "source": [
        "#run everytime running Random Forest\n",
        "#running the Random forest model again but with better parameters\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(max_depth=15,n_estimators=10)\n",
        "model.fit(x_train,y_train)\n",
        "y_train_pred = model.predict(x_train)\n",
        "y_test_pred = model.predict(x_cv_)\n",
        "\n",
        "train_fpr, train_tpr, thresholds = roc_curve(y_train, model.predict_proba(x_train)[:,1])\n",
        "test_fpr, test_tpr, thresholds = roc_curve(y_cv, model.predict_proba(x_cv_)[:,1])\n",
        "\n",
        "#Area under ROC curve\n",
        "print('Area under train roc {}'.format(auc(train_fpr, train_tpr)))\n",
        "print('Area under test roc {}'.format(auc(test_fpr, test_tpr)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiExU3Wu4eDe"
      },
      "source": [
        "#run everytime running the random forest\n",
        "#After getting nice accuracy, creating confusion matrix to analyse results better\n",
        "\n",
        "train_cf = confusion_matrix(y_train,y_train_pred)\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(train_cf,annot=True,annot_kws={\"size\": 16},fmt=\"0\")\n",
        "plt.title('Train confusion matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "cv_cf = confusion_matrix(y_cv,y_test_pred)\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(cv_cf,annot=True,annot_kws={\"size\": 16},fmt=\"0\")\n",
        "plt.title('Test confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN8QgoO9z5Ba"
      },
      "source": [
        "We already got our results with a decent accuracy with Random Forest model but to increase the accuracy let us do a coorelation analysis of database to understand how various features are dependent on each other, this will not only help us in increasing accuracy but will also help us in dropping less significant features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5UT8tzM0iWv"
      },
      "source": [
        "#run everytime\n",
        "#importing databases once again\n",
        "df_train = pd.read_csv('data/train_combined.csv')\n",
        "df_test = pd.read_csv('data/test_combined.csv')\n",
        "print(df_train.shape)\n",
        "print(df_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5XopQVC2EaV"
      },
      "source": [
        "#run everytime\n",
        "#A function to find out columns with missing values more than a particular %\n",
        "\n",
        "def top_missing_cols(df,n=10,thresh=80):\n",
        "    \"\"\"\n",
        "    returns missing columns in dataframe with missing values percent > thresh\n",
        "    if n=None. It will gave whole dataframe with missing values percent > thresh\n",
        "    \"\"\"\n",
        "    \n",
        "    dff = (df.isnull().sum()/df.shape[0])*100\n",
        "    dff = dff.reset_index()\n",
        "    dff.columns = ['col','missing_percent']\n",
        "    dff = dff.sort_values(by=['missing_percent'],ascending=False).reset_index(drop=True)\n",
        "    print(f'There are {df.isnull().any().sum()} columns in this dataset with missing values.')\n",
        "    print(f'There are {dff[dff[\"missing_percent\"] > thresh].shape[0]} columns with missing percent values than {thresh}%')\n",
        "    if n:\n",
        "        return dff.head(n)\n",
        "    else:\n",
        "        return dff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2hcSUaOAlUC"
      },
      "source": [
        "#run everytime\n",
        "#we will only run Coorelation Analysis on columns having more than 50% missing values\n",
        "\n",
        "df_missing = top_missing_cols(df_train,n=None,thresh=50)\n",
        "# Taking all column with missing percen > 50\n",
        "missing_cols = df_missing['col']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARfZN1R0PfXC"
      },
      "source": [
        "#run everytime\n",
        "# we will take all columns and group them based on missing percentage\n",
        "\n",
        "nan_dict = {}\n",
        "for col in missing_cols:\n",
        "    count = df_train[col].isnull().sum()\n",
        "    try:\n",
        "        nan_dict[count].append(col)\n",
        "    except:\n",
        "        nan_dict[count] = [col]\n",
        "        \n",
        "for k,v in nan_dict.items():\n",
        "    print(f'#####' * 4)\n",
        "    print(f'NAN count = {k} percent: {(int(k)/df_train.shape[0])*100} %')\n",
        "    print(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09nHxCsBQROa"
      },
      "source": [
        "**Reducing V-Cols**\n",
        "\n",
        "From our Xgboost model, after plotting feature importance we came to know that all V cols do not contribute much to the model. Also on analysis, we found that there exists a strong correlation between several ‘Vxx’ columns. So we decided to reduce the number of columns by doing a correlation analysis.\n",
        "\n",
        "**Procedure**\n",
        "* Group columns based on a number of missing values Eg. if there are 4 columns v1,v2,v3, and v4. If v1 and v3 have 56 missing values and v2 have 21 and v4 have 5 missing values, we have 3 groups [‘v1’,’v3'], [‘v2’] and [‘v4’]\n",
        "* In each group, For each column in that group find the correlation with other columns and take only columns with a correlation coefficient > 0.75. Take the largest list with common elements as a subgroup. Each group contains several subgroups. For eg: if we have [[v1,v2],[v6],[v1,v4,v2,v5],[v5,v4]] ,our output will be [[v1,v2,v4,v5],[v6]]. Now from each subgroup choose the column with the most number of unique values. For eg, In subgroup [v1,v2,v4,v5], let v2 have the most unique values. So out output becomes [v2,v6].\n",
        "\n",
        "We know that columns ‘V35’, ‘V40’, ‘V41’, ‘V39’, ‘V38’, ‘V51’, ‘V37’, ‘V52’, ‘V36’, ‘V50’, ‘V48’, ‘V42’,‘V43’, ‘V44’, ‘V46’, ‘V47’, ‘V45’, ‘V49’ 168969 missing values. So they form a group. Now we will plot a correlation matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arHLsAYAQyI6"
      },
      "source": [
        "#run everytime\n",
        "#A function to get the coorelation matrix between a set of columns\n",
        "\n",
        "def coorelation_analysis(cols,title='Coorelation Analysis',size=(12,12)):\n",
        "    cols = sorted(cols)\n",
        "    fig,axes = plt.subplots(1,1,figsize=size)\n",
        "    df_corr = df_train[cols].corr()\n",
        "    sns.heatmap(df_corr,annot=True,cmap='RdBu_r')\n",
        "    axes.title.set_text(title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9S4RqrNRCDV"
      },
      "source": [
        "#run everytime\n",
        "#A function to get all the columns having significance lower than 0.75\n",
        "\n",
        "def reduce_groups(grps):\n",
        "    '''\n",
        "    determining column that have more unique values among a group of atttributes\n",
        "    '''\n",
        "    use = []\n",
        "    for col in grps:\n",
        "        max_unique = 0\n",
        "        max_index = 0\n",
        "        for i,c in enumerate(col):\n",
        "            n = df_train[c].nunique()\n",
        "            if n > max_unique:\n",
        "                max_unique = n\n",
        "                max_index = i\n",
        "        use.append(col[max_index])\n",
        "    return use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo1aCej2W66k"
      },
      "source": [
        "#run once\n",
        "#An example of Coorelation Analysis\n",
        "\n",
        "cols = ['V35', 'V40', 'V41', 'V39', 'V38', 'V51', 'V37', 'V52', 'V36', 'V50', 'V48', 'V42',\n",
        " 'V43', 'V44', 'V46', 'V47', 'V45', 'V49']\n",
        "coorelation_analysis(cols,title='Coorelation Analysis: V35-V52',size=(12,12))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGD7oefMXBIM"
      },
      "source": [
        "#run once\n",
        "#Getting pairs which can be reduced from previous analysis\n",
        "\n",
        "pairs = [['V35','V36'],['V37','V38'],['V39','V40','V42','V43','V50','V51','V52'],['V41'],\n",
        "         ['V44','V45'],['V46','V47'],['V48','V49']]\n",
        "red_cols = reduce_groups(pairs)\n",
        "red_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEx9RcmdXarL"
      },
      "source": [
        "#run once\n",
        "#After running Coorelation analysis over all VCols the columns that can be reduced are\n",
        "\n",
        "reduced_vcols = ['V1', 'V3', 'V4', 'V6', 'V8', 'V11', 'V13', 'V14', 'V17', 'V20', \n",
        " 'V23', 'V26', 'V27', 'V30', 'V36', 'V37', 'V40', 'V41', 'V44', 'V47', 'V48', 'V54', 'V56', 'V59', \n",
        " 'V62', 'V65', 'V67', 'V68', 'V70', 'V76', 'V78', 'V80', 'V82', 'V86', 'V88', 'V89', 'V91', 'V96', \n",
        " 'V98', 'V99', 'V104', 'V107', 'V108', 'V111', 'V115', 'V117', 'V120', 'V121', 'V123', 'V124', 'V127', \n",
        " 'V129', 'V130', 'V136', 'V138', 'V139', 'V142', 'V147', 'V156', 'V162', 'V165', 'V160', 'V166', 'V178',\n",
        " 'V176', 'V173', 'V182', 'V187', 'V203', 'V205', 'V207', 'V215', 'V169', 'V171', 'V175', 'V180', 'V185', \n",
        " 'V188', 'V198', 'V210', 'V209', 'V218', 'V223', 'V224', 'V226', 'V228', 'V229', 'V235', 'V240', 'V258', \n",
        " 'V257', 'V253', 'V252', 'V260', 'V261', 'V264', 'V266', 'V267', 'V274', 'V277', 'V220', 'V221', 'V234', \n",
        " 'V238', 'V250', 'V271', 'V294', 'V284', 'V285', 'V286', 'V291',\n",
        " 'V297', 'V303', 'V305', 'V307', 'V309', 'V310', 'V320', 'V281', 'V283', 'V289', 'V296', 'V301', 'V314', 'V332', 'V325', 'V335', 'V338']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpE52FQGZbXL"
      },
      "source": [
        "We did a similar coorelation analysis for ID, C Cols and M cols and created a list of all the columns needed to be reduced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4klJVJMBZ12U"
      },
      "source": [
        "#run everytime running XGboost\n",
        "# column details\n",
        "cat_cols = (['ProductCD'] + \n",
        "            ['card%d' % i for i in range(1, 7)] + \n",
        "            ['addr1', 'addr2', 'P_emaildomain', 'R_emaildomain'] + \n",
        "            ['M%d' % i for i in range(1, 10)] + \n",
        "            ['DeviceType', 'DeviceInfo'] +\n",
        "            ['id_%d' % i for i in range(12, 39)])\n",
        "\n",
        "\n",
        "type_map = {c: str for c in cat_cols}\n",
        "df_train[cat_cols] = df_train[cat_cols].astype(type_map, copy=False)\n",
        "df_test[cat_cols] = df_test[cat_cols].astype(type_map, copy=False)\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "id_cols = ['TransactionID', 'TransactionDT']\n",
        "target = 'isFraud'\n",
        "\n",
        "numeric_cols =  [\n",
        "    'TransactionAmt', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', \n",
        "    'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', \n",
        "    'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', \n",
        "    'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', \n",
        "    'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', \n",
        "    'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', \n",
        "    'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', \n",
        "    'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', \n",
        "    'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', \n",
        "    'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', \n",
        "    'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', \n",
        "    'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', \n",
        "    'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', \n",
        "    'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', \n",
        "    'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', \n",
        "    'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', \n",
        "    'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', \n",
        "    'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', \n",
        "    'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', \n",
        "    'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', \n",
        "    'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', \n",
        "    'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', \n",
        "    'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', \n",
        "    'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', \n",
        "    'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', \n",
        "    'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', \n",
        "    'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', \n",
        "    'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', \n",
        "    'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', \n",
        "    'V337', 'V338', 'V339', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', \n",
        "    'id_09', 'id_10', 'id_11'\n",
        "]\n",
        "\n",
        "\n",
        "reduced_vcols = ['V1', 'V3', 'V4', 'V6', 'V8', 'V11', 'V13', 'V14', 'V17', 'V20', \n",
        " 'V23', 'V26', 'V27', 'V30', 'V36', 'V37', 'V40', 'V41', 'V44', 'V47', 'V48', 'V54', 'V56', 'V59', \n",
        " 'V62', 'V65', 'V67', 'V68', 'V70', 'V76', 'V78', 'V80', 'V82', 'V86', 'V88', 'V89', 'V91', 'V96', \n",
        " 'V98', 'V99', 'V104', 'V107', 'V108', 'V111', 'V115', 'V117', 'V120', 'V121', 'V123', 'V124', 'V127', \n",
        " 'V129', 'V130', 'V136', 'V138', 'V139', 'V142', 'V147', 'V156', 'V162', 'V165', 'V160', 'V166', 'V178',\n",
        " 'V176', 'V173', 'V182', 'V187', 'V203', 'V205', 'V207', 'V215', 'V169', 'V171', 'V175', 'V180', 'V185', \n",
        " 'V188', 'V198', 'V210', 'V209', 'V218', 'V223', 'V224', 'V226', 'V228', 'V229', 'V235', 'V240', 'V258', \n",
        " 'V257', 'V253', 'V252', 'V260', 'V261', 'V264', 'V266', 'V267', 'V274', 'V277', 'V220', 'V221', 'V234', \n",
        " 'V238', 'V250', 'V271', 'V294', 'V284', 'V285', 'V286', 'V291',\n",
        " 'V297', 'V303', 'V305', 'V307', 'V309', 'V310', 'V320', 'V281', 'V283', 'V289', 'V296', 'V301', 'V314', 'V332', 'V325', 'V335', 'V338']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2eqkhrAaH2T"
      },
      "source": [
        "#run everytime running XGboost\n",
        "#dropping columns to be reduced\n",
        "\n",
        "drop_cols = [col for col in df_train.columns if col[0] == 'V' and col not in reduced_vcols]\n",
        "\n",
        "print(f'dropping {len(drop_cols)} columns')\n",
        "df_train = df_train.drop(columns=drop_cols)\n",
        "df_test = df_test.drop(columns=drop_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Why1ac52aW2i"
      },
      "source": [
        "#run evverytime running XGBoost\n",
        "## Train and test split\n",
        "\n",
        "y_train = df_train['isFraud']\n",
        "X_train = df_train.drop(columns=['isFraud'])\n",
        "X_test = df_test.copy()\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBcL2d6maelP"
      },
      "source": [
        "#run everytime running XGBoost\n",
        "# Label encoding all cat features\n",
        "\n",
        "for col in X_train.columns:\n",
        "    \n",
        "    if col in cat_cols:\n",
        "        # label encode all cat columns\n",
        "        dff = pd.concat([X_train[col],X_test[col]])\n",
        "        dff,_ = pd.factorize(dff,sort=True)\n",
        "        if dff.max()>32000: \n",
        "            print(col,'needs int32 datatype')\n",
        "            \n",
        "        X_train[col] = dff[:len(X_train)].astype('int16')\n",
        "        X_test[col] = dff[len(X_train):].astype('int16')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8mp3Lh3alPK"
      },
      "source": [
        "#run everytime running XGBoost\n",
        "\n",
        "rem_cols = []\n",
        "rem_cols.extend(['TransactionDT','TransactionID'])\n",
        "\n",
        "cols = [col for col in X_train.columns if col not in rem_cols]\n",
        "len(cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haHcQaNcarN6"
      },
      "source": [
        "#run everytime running XGBoost\n",
        "# Scaling numeric features\n",
        "\n",
        "for col in cols:\n",
        "    if col not in cat_cols:\n",
        "        # min max scalar\n",
        "        dff = pd.concat([X_train[col],X_test[col]])\n",
        "        dff = (dff - dff.min())/(dff.max() - dff.min())\n",
        "        dff.fillna(-1,inplace=True)\n",
        "\n",
        "        X_train[col] = dff[:len(X_train)]\n",
        "        X_test[col] = dff[len(X_train):]\n",
        "\n",
        "del dff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCDfGhx-a6bO"
      },
      "source": [
        "#run everytime running XGBoost\n",
        "#Fitting the XGBoost model\n",
        "\n",
        "x_train = X_train[cols]\n",
        "x_test = X_test[cols]\n",
        "\n",
        "\n",
        "idx_train = x_train.index[:int(x_train.shape[0]*0.75)]  \n",
        "idx_validation = x_train.index[int(x_train.shape[0]*0.75):]\n",
        "    \n",
        "print(f'fitting model on {len(cols)} columns')\n",
        "clf = xgb.XGBClassifier( \n",
        "        n_estimators=2000,\n",
        "        max_depth=12, \n",
        "        learning_rate=0.02, \n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.4, \n",
        "        missing=-1, \n",
        "        eval_metric='auc',\n",
        "        tree_method='gpu_hist' \n",
        "          )\n",
        "model = clf.fit(x_train.loc[idx_train,cols], y_train[idx_train], \n",
        "            eval_set=[(x_train.loc[idx_validation,cols],y_train[idx_validation])],\n",
        "            verbose=50, early_stopping_rounds=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R0KZAsKbGIw"
      },
      "source": [
        "#run everytime running XGBoost\n",
        "#Making predictions with XGboost on training and testing database\n",
        "\n",
        "y_train_pred = model.predict(x_train.iloc[idx_train])\n",
        "y_test_pred = model.predict(x_train.iloc[idx_validation])\n",
        "\n",
        "train_fpr, train_tpr, thresholds = roc_curve(y_train.iloc[idx_train], model.predict_proba(x_train.iloc[idx_train])[:,1])\n",
        "test_fpr, test_tpr, thresholds = roc_curve(y_train.iloc[idx_validation], model.predict_proba(x_train.iloc[idx_validation])[:,1])\n",
        "\n",
        "#Area under ROC curve\n",
        "print('Area under train roc {}'.format(auc(train_fpr, train_tpr)))\n",
        "print('Area under test roc {}'.format(auc(test_fpr, test_tpr)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAqi7stSbW0L"
      },
      "source": [
        "#run everytime running XGBoost\n",
        "#Creating Confusion matrix for analysis\n",
        "\n",
        "train_cf = confusion_matrix(y_train.iloc[idx_train],y_train_pred)\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(train_cf,annot=True,annot_kws={\"size\": 16},fmt=\"0\")\n",
        "plt.title('Train confusion matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "cv_cf = confusion_matrix(y_train.iloc[idx_validation],y_test_pred)\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(cv_cf,annot=True,annot_kws={\"size\": 16},fmt=\"0\")\n",
        "plt.title('Test confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSXxJLJD3umq"
      },
      "source": [
        "#it is usually preferred to reload the database for new model so we can delete recent one for now.\n",
        "\n",
        "del df_train,df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeNFUwxh34KI"
      },
      "source": [
        "**Engineering D Cols**\n",
        "\n",
        "From Kaggle's discussions, I came to know that D1 is the day since credit card usage began. Subtracting this from Transaction Day will make it an almost constant value per client. I did the same approach for all the D cols. To check the newly created features are helpful or not I did a forward feature selection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-62GxBaA4BqS"
      },
      "source": [
        "#run everytime\n",
        "#importing database again\n",
        "\n",
        "df_train_transaction = pd.read_csv('data/train_transaction.csv')\n",
        "df_train_identity = pd.read_csv('data/train_identity.csv')\n",
        "print(f'Shape of transaction train data: {df_train_transaction.shape}')\n",
        "print(f'Shape of identity train data: {df_train_identity.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xQ7i66N5Pe8"
      },
      "source": [
        "#run everytime \n",
        "\n",
        "# column details\n",
        "cat_cols = (['ProductCD'] + \n",
        "            ['card%d' % i for i in range(1, 7)] + \n",
        "            ['addr1', 'addr2', 'P_emaildomain', 'R_emaildomain'] + \n",
        "            ['M%d' % i for i in range(1, 10)] + \n",
        "            ['DeviceType', 'DeviceInfo'] +\n",
        "            ['id_%d' % i for i in range(12, 39)])\n",
        "\n",
        "\n",
        "type_map = {c: str for c in cat_cols}\n",
        "df_train[cat_cols] = df_train[cat_cols].astype(type_map, copy=False)\n",
        "df_test[cat_cols] = df_test[cat_cols].astype(type_map, copy=False)\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "id_cols = ['TransactionID', 'TransactionDT']\n",
        "target = 'isFraud'\n",
        "\n",
        "numeric_cols =  [\n",
        "    'TransactionAmt', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', \n",
        "    'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', \n",
        "    'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', \n",
        "    'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', \n",
        "    'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', \n",
        "    'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', \n",
        "    'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', \n",
        "    'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', \n",
        "    'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', \n",
        "    'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', \n",
        "    'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', \n",
        "    'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', \n",
        "    'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', \n",
        "    'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', \n",
        "    'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', \n",
        "    'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', \n",
        "    'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', \n",
        "    'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', \n",
        "    'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', \n",
        "    'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', \n",
        "    'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', \n",
        "    'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', \n",
        "    'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', \n",
        "    'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', \n",
        "    'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', \n",
        "    'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', \n",
        "    'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', \n",
        "    'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', \n",
        "    'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', \n",
        "    'V337', 'V338', 'V339', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', \n",
        "    'id_09', 'id_10', 'id_11'\n",
        "]\n",
        "\n",
        "v_cols_included = ['V1', 'V3', 'V4', 'V6', 'V8', 'V11', 'V13', 'V14', 'V17', 'V20', \n",
        " 'V23', 'V26', 'V27', 'V30', 'V36', 'V37', 'V40', 'V41', 'V44', 'V47', 'V48', 'V54', 'V56', 'V59', \n",
        " 'V62', 'V65', 'V67', 'V68', 'V70', 'V76', 'V78', 'V80', 'V82', 'V86', 'V88', 'V89', 'V91', 'V96', \n",
        " 'V98', 'V99', 'V104', 'V107', 'V108', 'V111', 'V115', 'V117', 'V120', 'V121', 'V123', 'V124', 'V127', \n",
        " 'V129', 'V130', 'V136', 'V138', 'V139', 'V142', 'V147', 'V156', 'V162', 'V165', 'V160', 'V166', 'V178',\n",
        " 'V176', 'V173', 'V182', 'V187', 'V203', 'V205', 'V207', 'V215', 'V169', 'V171', 'V175', 'V180', 'V185', \n",
        " 'V188', 'V198', 'V210', 'V209', 'V218', 'V223', 'V224', 'V226', 'V228', 'V229', 'V235', 'V240', 'V258', \n",
        " 'V257', 'V253', 'V252', 'V260', 'V261', 'V264', 'V266', 'V267', 'V274', 'V277', 'V220', 'V221', 'V234', \n",
        " 'V238', 'V250', 'V271', 'V294', 'V284', 'V285', 'V286', 'V291',\n",
        " 'V297', 'V303', 'V305', 'V307', 'V309', 'V310', 'V320', 'V281', 'V283', 'V289', 'V296', 'V301', 'V314', 'V332', 'V325', 'V335', 'V338']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6iicXm45hQB"
      },
      "source": [
        "#run everytime\n",
        "# droping v cols \n",
        "\n",
        "drop_cols = [col for col in df_train.columns if col[0] == 'V' and col not in v_cols_included]\n",
        "\n",
        "print(f'dropping {len(drop_cols)} columns')\n",
        "df_train = df_train.drop(columns=drop_cols)\n",
        "df_test = df_test.drop(columns=drop_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnorwVTW5oSa"
      },
      "source": [
        "#run everytime\n",
        "## Train and test split\n",
        "\n",
        "y_train = df_train['isFraud']\n",
        "X_train = df_train.drop(columns=['isFraud'])\n",
        "X_test = df_test.copy()\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2jokE6W5u40"
      },
      "source": [
        "#run everytime\n",
        "# Label encoding all cat features\n",
        "\n",
        "for col in X_train.columns:\n",
        "    if col in cat_cols:\n",
        "        # label encode all cat columns\n",
        "        dff = pd.concat([X_train[col],X_test[col]])\n",
        "        dff,_ = pd.factorize(dff,sort=True)\n",
        "        if dff.max()>32000: \n",
        "            print(col,'needs int32 datatype')\n",
        "            \n",
        "        X_train[col] = dff[:len(X_train)].astype('int16')\n",
        "        X_test[col] = dff[len(X_train):].astype('int16')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmcybxug51rX"
      },
      "source": [
        "TransactionDT and TransactionID,\n",
        "Both columns are unique. One is time-related information and the other is a unique id. Adding this to the model doesn't make much sense. So we removed those features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOKHOvVH58I5"
      },
      "source": [
        "#run everytime\n",
        "#removing the 2 columns \n",
        "\n",
        "rem_cols = []\n",
        "rem_cols.extend(['TransactionDT','TransactionID'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sxB3bRH6HjV"
      },
      "source": [
        "#run everytime\n",
        "#defining XGBoost model with all the parameters\n",
        "\n",
        "def model(x_train,y_train,cols):\n",
        "    idx_train = x_train.index[:int(x_train.shape[0]*0.75)]  \n",
        "    idx_validation = x_train.index[int(x_train.shape[0]*0.75):]\n",
        "    \n",
        "    print(f'fitting model on {len(current_cols)} columns')\n",
        "    clf = xgb.XGBClassifier( \n",
        "        n_estimators=2000,\n",
        "        max_depth=12, \n",
        "        learning_rate=0.02, \n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.4, \n",
        "        missing=-1, \n",
        "        eval_metric='auc',\n",
        "        tree_method='gpu_hist' \n",
        "          )\n",
        "    model = clf.fit(x_train.loc[idx_train,cols], y_train[idx_train], \n",
        "            eval_set=[(x_train.loc[idx_validation,cols],y_train[idx_validation])],\n",
        "            verbose=50, early_stopping_rounds=100)\n",
        "    del clf\n",
        "    return model.best_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhkN0vT76Pqz"
      },
      "source": [
        "#run everytime\n",
        "# NORMALIZE D COLUMNS\n",
        "for i in range(1,16):\n",
        "    X_train['D'+str(i)+'n'] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n",
        "    X_test['D'+str(i)+'n'] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65NloLjD6TLL"
      },
      "source": [
        "#run everytime\n",
        "# Scaling numeric features\n",
        "cols = [col for col in X_train.columns if col not in rem_cols]\n",
        "for col in cols:\n",
        "    if col not in cat_cols:\n",
        "        # min max scalar\n",
        "        dff = pd.concat([X_train[col],X_test[col]])\n",
        "        dff = (dff - dff.min())/(dff.max() - dff.min())\n",
        "        dff.fillna(-1,inplace=True)\n",
        "\n",
        "        X_train[col] = dff[:len(X_train)]\n",
        "        X_test[col] = dff[len(X_train):]\n",
        "\n",
        "del dff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o419lfRY6YB3"
      },
      "source": [
        "del df_train,df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0wA_dd46fWM"
      },
      "source": [
        "#run everytime\n",
        "#replacing the old D cols with new normalized ones\n",
        "\n",
        "d_norm_cols = ['D'+str(i)+'n' for i in range(1,16)]\n",
        "d_cols = ['D'+str(i) for i in range(1,16)]\n",
        "old_cols = [col for col in X_train.columns if col not in rem_cols and col not in d_norm_cols]\n",
        "len(old_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mND-NlJd6nKp"
      },
      "source": [
        "#run everytime\n",
        "#choosing the only required D cols and dropping the rest of them by checking accuracy of model with all\n",
        "\n",
        "len_new_cols = 16\n",
        "best_cols = []\n",
        "best_score = 0.9231\n",
        "iteration = 1\n",
        "\n",
        "\n",
        "while iteration < len_new_cols:\n",
        "    for i in range(15):\n",
        "        print(f'Added new column {d_norm_cols[i]}')\n",
        "        if d_norm_cols[i] not in best_cols:\n",
        "            current_cols = old_cols.copy()\n",
        "            # adding and removing Di\n",
        "        \n",
        "            best_cols.append(d_norm_cols[i])\n",
        "            \n",
        "            print(f'working with {best_cols}')\n",
        "\n",
        "            current_cols.extend([c for c in best_cols])\n",
        "\n",
        "            for c in best_cols:\n",
        "              if c[:-1] in current_cols:  #if D1 in current cols\n",
        "                current_cols.remove(c[:-1])\n",
        "\n",
        "            \n",
        "            print('--'*40)\n",
        "            current_score = model(X_train,y_train,current_cols)\n",
        "\n",
        "            if current_score > best_score:\n",
        "                print(f'score improved on adding {d_norm_cols[i]} new best score {current_score}')\n",
        "                iteration_best_col = d_norm_cols[i]\n",
        "                best_score = current_score\n",
        "\n",
        "            else:\n",
        "                current_cols.remove(d_norm_cols[i])\n",
        "                current_cols.append(d_norm_cols[i][:-1])\n",
        "\n",
        "            best_cols.remove(d_norm_cols[i])\n",
        "        else:\n",
        "          print(f'Column {d_norm_cols[i]} already present')    \n",
        "\n",
        "    \n",
        "    iteration += 1\n",
        "    if iteration_best_col is not None:\n",
        "        best_cols.append(iteration_best_col)\n",
        "        iteration_best_col = None\n",
        "        print(f'Best col in this iteration {iteration_best_col}, current best cols {best_cols}, score {best_score}')\n",
        "                  \n",
        "    else:\n",
        "        print('None of the columns improving the score in this iteration')\n",
        "        break\n",
        "    print('###'*40)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O8rrFIf66ib"
      },
      "source": [
        " gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}